{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db50cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import ipywidgets as widgets\n",
    "import scipy\n",
    "from scipy import special\n",
    "import time\n",
    "from scipy.signal import convolve as conv\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "plt.style.use(plt.style.available[20])\n",
    "plt.style.use(\"https://github.com/comp-neural-circuits/intro-to-comp-neuro/raw/dev/plots_style.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bc9b9",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "Today we want to investigate some RL algorithms and how they can be used to train an agent that moves around in a grid environment.\n",
    "\n",
    "## The cliff\n",
    "<div>\n",
    "<img src=\"https://github.com/comp-neural-circuits/intro-to-comp-neuro/raw/dev/notebooks/Exc_12/static/gridworld_setup.png\" width=\"550\"/>\n",
    "</div>\n",
    "\n",
    "The agent should get to the goal with as few steps as possible without falling into the cliff.\n",
    "\n",
    "In order to invesigate the agents behavior we first setup the viusal representation of the environment:\n",
    "\n",
    "(You do not have to go through the code, you can also just execute the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b7454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_grid_world(ax=None):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "\n",
    "\n",
    "    ax.grid(linewidth = 1.4, c='k')\n",
    "    ax.set(\n",
    "        xticks = [0.5+ii for ii in range(-1,12)],\n",
    "        yticks = [0.5 + ii for ii in range(-1,4)],\n",
    "        xlim=[-0.52,11.53],\n",
    "        ylim = [-0.51,3.52])\n",
    "\n",
    "\n",
    "    ax.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False) # labels along the bottom edge are off\n",
    "    ax.tick_params(\n",
    "        axis='y',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        right=False,         # ticks along the top edge are off\n",
    "        labelleft=False) # labels along the bottom edge are off\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    rect=mpatches.Rectangle((0.51,-0.48),9.98,0.96, \n",
    "                            fill=True,\n",
    "                            color=\"#bdbdbd\",zorder=50)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    ax.annotate('S',(0,0), fontweight='bold',fontsize=23,ha='center',va='center',zorder=80)\n",
    "    ax.annotate('G',(11,0), fontweight='bold',fontsize=23,ha='center',va='center',zorder=80)\n",
    "    ax.annotate('The Cliff',(5.5,-0.1), fontsize=23,ha='center',va='center',zorder=80)\n",
    "    return ax\n",
    "show_grid_world();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89376fd9",
   "metadata": {},
   "source": [
    "## Moving in the environment\n",
    "\n",
    "The agent is able to move in the environment\n",
    "\n",
    "Below you can use the widget to see how we can map the states to a position in the 2d grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98962bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2d5dba12a344c48db84cac770aa161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=23, description='state', max=47), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def move_states(state):\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    show_grid_world(ax)\n",
    "    agent = ax.scatter(state%12,state//12,s=800,c='b',zorder=60)\n",
    "    print (f'state {state}:      x = {state%12}, y = {state//12}')\n",
    "\n",
    "widgets.interactive(move_states, state = (0,47,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242ffa4",
   "metadata": {},
   "source": [
    "## programming the environment\n",
    "\n",
    "We need to define how the agent moves in the environment, following the four possible actions.\n",
    "\n",
    "The agent can either move right (0), down (1), left (2) or up (3)\n",
    "If the agent moves against a well, it will stay at the current state.\n",
    "We implement this with if-cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state, action):\n",
    "        \n",
    "    if action == 0:  # move right\n",
    "        next_state = state + 1\n",
    "        if state % 12 == 11:  # right border\n",
    "            next_state = state\n",
    "            \n",
    "    elif action == 1:  # move down\n",
    "        next_state = state - 12\n",
    "        if state <= 11:  # bottom border\n",
    "            next_state = state\n",
    "    \n",
    "    elif action == 2:  # move left\n",
    "        next_state = state - 1\n",
    "        if state % 12 == 0:  # left border\n",
    "            next_state = state\n",
    "            \n",
    "    elif action == 3:  # move up\n",
    "        next_state = state + 12\n",
    "        if state >= 36:  # top border\n",
    "            next_state = state\n",
    "    \n",
    "    else:\n",
    "        print(\"Action must be between 0 and 3.\")\n",
    "        return None\n",
    "\n",
    "    return int(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25fbd2",
   "metadata": {},
   "source": [
    "## Test whether the agent moves as we wish\n",
    "We can now test whether the actions have the correct effect in the world, if you execute the cell below the agent will execute a couple of random actions in the environment.\n",
    "For now we only care for the fact that an action (indicated by an arrow) should lead to the state we expect in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d64dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_visualization(state = 0):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    show_grid_world(ax)\n",
    "    state_2d = [state%12,state//12] \n",
    "    agent = ax.scatter(state_2d[0],state_2d[1],s=800,c='b',zorder=60)\n",
    "    arrow = ax.annotate(\"\",\n",
    "                  xy=(0, 1), xycoords='data',\n",
    "                  xytext=(1, 1), textcoords='data',\n",
    "                  arrowprops=dict(#arrowstyle=\"-\",\n",
    "                                  connectionstyle=\"arc3,rad=0.\",\n",
    "                                  linewidth=4, fc='k'),\n",
    "                        zorder = 100,\n",
    "                        annotation_clip = False\n",
    "                  )\n",
    "    \n",
    "    return fig, ax, agent, arrow\n",
    "\n",
    "\n",
    "state = 0\n",
    "n_steps = 100\n",
    "all_actions = []\n",
    "all_states = [state]\n",
    "\n",
    "for ii in range(n_steps):\n",
    "   \n",
    "    action = np.random.choice(4)\n",
    "    state = take_action(state, action)\n",
    "    \n",
    "    all_actions.append(action)\n",
    "    all_states.append(state)\n",
    "    \n",
    "    \n",
    "def visualize_taken_actions(agent, all_states, all_actions, state_index=0):\n",
    "   \n",
    "    state = all_states[state_index]\n",
    "    action = all_actions[state_index]\n",
    "    \n",
    "    x = state%12\n",
    "    y =state//12\n",
    "  \n",
    "    if action == 0:\n",
    "        target_x, target_y = x + 1, y\n",
    "    if action == 1:\n",
    "        target_x, target_y = x, y - 1\n",
    "    if action == 2:\n",
    "        target_x, target_y = x - 1, y\n",
    "    if action == 3:\n",
    "        target_x, target_y = x, y + 1\n",
    "    \n",
    "    fig, ax, agent, arrow = setup_visualization(state )\n",
    "    \n",
    "    arrow.xy = (target_x ,target_y )\n",
    "    arrow.set_position([x,y])\n",
    "    agent.set_offsets([x,y])\n",
    "    \n",
    "\n",
    "widgets.interactive(visualize_taken_actions, state_index = (0,len(all_states)-2,1), \n",
    "                    agent=widgets.fixed(agent),\n",
    "                    all_states=widgets.fixed(all_states),\n",
    "                    all_actions=widgets.fixed(all_actions),\n",
    "                    arrow = widgets.fixed(arrow),\n",
    "                    \n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87327d58",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "Next we need to define the rewards the agent recieves for reaching a certain state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state):\n",
    "    \n",
    "    if state >= 1 and state <= 10: # cliff\n",
    "        return -100\n",
    "    elif state == 11: # goal\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d176d3",
   "metadata": {},
   "source": [
    "## Episode\n",
    "Now we can define an episode. \n",
    "\n",
    "Here, we look at an episodic task. This means that the agent can reach an endpoint (in this case the goal or the cliff) and thereby stops the current episode. However, it can be started again ...\n",
    "\n",
    "### Task 1\n",
    "Go throught he code and make sure that you understand how the episode is implemented and what the stopping conditions are.\n",
    "\n",
    "Then execute the cell and see how well the agent performs with the random policy we provide.\n",
    "(reminder: the policy describes the mapping from a state to an action. In our case, the action is just randomly selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(**kwargs):\n",
    "    return np.random.choice(4)\n",
    "\n",
    "\n",
    "def run_episode(policy, state=0):\n",
    "        \n",
    "    all_actions = []\n",
    "    all_states = [state]\n",
    "    \n",
    "    max_steps = 2000\n",
    "    reward_sum = 0\n",
    "        \n",
    "    for t in range(max_steps):\n",
    "        # choose next action\n",
    "        action = policy(state=state,q = state_action_values[state])\n",
    "        all_actions.append(action)\n",
    "\n",
    "        # observe outcome of action on environment\n",
    "        next_state = take_action(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        # sum rewards obtained\n",
    "        reward_sum += reward\n",
    "\n",
    "        if reward == -100 or reward == 0:\n",
    "            break  # episode ends\n",
    "        state = next_state\n",
    "        \n",
    "        all_states.append(state)\n",
    "     \n",
    "    return all_states, all_actions, reward_sum\n",
    "    \n",
    "\n",
    "state = 0 \n",
    "all_states, all_actions, reward_sum = run_episode(policy=random_policy, state = state)\n",
    "ax.set_title(f'Reward Sum: {reward_sum}')\n",
    "\n",
    "\n",
    "    \n",
    "widgets.interactive(visualize_taken_actions, state_index = (0,len(all_states)-1,1), \n",
    "                    agent=widgets.fixed(agent),\n",
    "                    all_states=widgets.fixed(all_states),\n",
    "                    all_actions=widgets.fixed(all_actions)\n",
    "                    \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558390b1",
   "metadata": {},
   "source": [
    "We can see that the random approach is not very efficient. We need a better policy!\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Can you program a policy that always reaches the goal? Be creative!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e6744",
   "metadata": {},
   "source": [
    "### [Solution 2](https://raw.githubusercontent.com/comp-neural-circuits/intro-to-comp-neuro/dev/notebooks/Exc_12/solutions/ade42d1ac53626e9723fe2ae7ca52865.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367f498",
   "metadata": {},
   "source": [
    "## State-action values\n",
    "\n",
    "We now want to learn the value function of the environment. More specifically, in this case we are going to learn the state-action values. \n",
    "\n",
    "First, we write a function to display the state-action values in the grid world.\n",
    "\n",
    "Every state has four state-action values and we are especially interested in the best one per state. Therefore we will display the values as colored triangles and the best action of each state will be a circle:\n",
    "\n",
    "(You don't have to necessarily go through the code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state_action_values_in_grid(ax = None, min_val = 0, max_val = 0, state_action_values=np.random.rand(48,4)):\n",
    "        \n",
    "        fig, (ax, cax) = plt.subplots(2,1, gridspec_kw={'height_ratios': [15,1]})\n",
    "        \n",
    "        \n",
    "        cmap = mpl.cm.hot\n",
    "        norm = mpl.colors.Normalize(vmin=min_val, vmax=max_val)\n",
    "\n",
    "#         self.state_action_values = -100*np.random.rand(self.n_states,4)\n",
    "        \n",
    "        ax = show_grid_world(ax=ax)\n",
    "        cax.grid(False)\n",
    "        cb = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "        cax.set_title(\"State-action value\")\n",
    "        \n",
    "        # the four triangle paths for the four actions\n",
    "        triangle_paths = np.array([\n",
    "            [[0.55,0.5],[0.95,0.95],[0.95,0.05]],\n",
    "            [[0.5,0.45],[0.9,0.05],[0.1,0.05]],\n",
    "            [[0.45,0.5],[0.05,0.05],[0.05,0.9]],\n",
    "            [[0.5,0.55],[0.05,0.95],[0.9,0.95]]           \n",
    "        ])\n",
    "        \n",
    "        for state in range(state_action_values.shape[0]):\n",
    "            \n",
    "            for action in range(4):\n",
    "                \n",
    "                state_2d = np.array([state%12,state//12]).astype(float)\n",
    "                \n",
    "                path = triangle_paths[action]+state_2d - np.array([0.5,0.5])\n",
    "                # convert the state-value to color \n",
    "                color = cmap(norm(state_action_values[state,action]))\n",
    "                \n",
    "                if np.argmax(state_action_values[state]) == action:\n",
    "                    ax.scatter(*np.mean(path,axis=0),color=color, edgecolor='k',linewidth=0.5, s=60)\n",
    "                else:\n",
    "                    triang=mpatches.Polygon(path,\n",
    "                                    fill=True,\n",
    "                                    color=color,zorder=10)\n",
    "                    \n",
    "                    ax.add_patch(triang)\n",
    "        plt.tight_layout()\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303b4dc",
   "metadata": {},
   "source": [
    "When executing the cell below you can look at an example (just random values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_state_action_values_in_grid(ax = None, min_val = 0, max_val = 1, state_action_values=np.random.rand(48,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc29175",
   "metadata": {},
   "source": [
    "## The policy ($\\epsilon$-greedy)\n",
    "\n",
    "In order to perform better (and to learn faster), we need to have a policy that can make use of the state-action values (and is better than the random policy).\n",
    "\n",
    "A policy that is often used is called $\\epsilon$-greedy\n",
    "\n",
    "In this policy, we usually take the best action, but in a fraction $\\epsilon$ we select a random action. \n",
    "This ensures that the agent keeps exploring, but on average it tries to select the best possible action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91065bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, epsilon):\n",
    "    \n",
    "    if np.random.random() > epsilon:\n",
    "        action = np.argmax(q)\n",
    "    else:\n",
    "        action = np.random.choice(len(q))\n",
    "\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3f4d1",
   "metadata": {},
   "source": [
    "## The learning rule\n",
    "\n",
    "Even more important than the policy is the learning rule (algorithm) that we want to use to update the state-action values.\n",
    "\n",
    "We learned a couple of variants in the lecture, here we select q-learning.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/comp-neural-circuits/intro-to-comp-neuro/raw/dev/notebooks/Exc_12/static/q_learning.png\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Make sure that you understand how the equation above is implemented in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(state, action, reward, next_state, state_action_values, params):\n",
    "    # Q-value of current state-action pair\n",
    "    q = state_action_values[state, action]\n",
    "    \n",
    "    if reward in [-100,0]: # this means the episode ends\n",
    "        max_next_q = 0\n",
    "    else:\n",
    "        max_next_q = np.max(state_action_values[next_state])\n",
    "\n",
    "    # write the expression to compute the TD error\n",
    "    td_error = reward + params['gamma'] * max_next_q - q\n",
    "    # write the expression that updates the Q-value for the state-action pair\n",
    "    state_action_values[state, action] = q + params['alpha'] * td_error\n",
    "\n",
    "    return state_action_values # we also return the state_values (although not changed) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6084ca",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Now we have all the ingredients to implement the function that allows the agent to learn to move in the environment.\n",
    "\n",
    "\n",
    "1 - we initiate the state-action values (just set all to one) \n",
    "(question: What could be the advantage if we initiate them all to -200?)\n",
    "\n",
    "2 - We loop through all the episodes we want to run for our learning\n",
    "\n",
    "3 - in each episode we select and action based on the policy\n",
    "\n",
    "4 - we take that action (and move to the next state)\n",
    "\n",
    "5 - we receive a reward \n",
    "\n",
    "6 - we update the state value based on q-learning\n",
    "\n",
    "7 - we check whether the episode ended\n",
    "\n",
    "make sure that you understand the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_environment(n_episodes, params, max_steps=400):\n",
    "               \n",
    "    # initiate the state-action values\n",
    "    state_action_values = np.ones((48,4))\n",
    "\n",
    "    # Loop over episodes\n",
    "    for episode in range(n_episodes):\n",
    "        state = 0  # initialize state, we start at state 0\n",
    "\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            # choose next action\n",
    "            action = epsilon_greedy(q = state_action_values[state], \n",
    "                             epsilon = params['epsilon'])\n",
    "            # observe outcome of action on environment\n",
    "            next_state = take_action(state, action)\n",
    "            reward = get_reward(next_state)\n",
    "\n",
    "            # update value function\n",
    "            state_action_values = q_learning(\n",
    "                state, \n",
    "                action, \n",
    "                reward, \n",
    "                next_state, \n",
    "                state_action_values, \n",
    "                params\n",
    "            )\n",
    "\n",
    "            if reward in [-100, 0]:\n",
    "                break  # episode ends\n",
    "            state = next_state\n",
    "    return state_action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f3cf3",
   "metadata": {},
   "source": [
    "## Execute the learning\n",
    "\n",
    "we can now run the learning algorithm and look at the learned state-action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'epsilon': 0.1,  # epsilon-greedy policy\n",
    "  'alpha': 0.1,  # learning rate\n",
    "  'gamma': 0.8,  # discount factor\n",
    "}\n",
    "state_action_values = learn_environment(n_episodes=400, params=params, max_steps=400)\n",
    "show_state_action_values_in_grid(ax = None, min_val = -8, max_val = 1, state_action_values=state_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9eb0e",
   "metadata": {},
   "source": [
    "We can now see a couple of things from the learned values:\n",
    "\n",
    "1 - The actions that lead into the cliff are learned to be extremely bad\n",
    "\n",
    "2 - can you see the path the agent would take if it follows the greedy policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e2d7e",
   "metadata": {},
   "source": [
    "## Greedy policy\n",
    "\n",
    "We can now use the greedy policy to select the best action at every timestep. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eaee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(q, **kwargs):\n",
    "    action = np.argmax(q)\n",
    "    return action\n",
    "\n",
    "\n",
    "state = 0 \n",
    "all_states, all_actions, reward_sum = run_episode(policy=greedy, state = state)\n",
    "ax.set_title(f'Reward Sum: {reward_sum}')\n",
    "\n",
    "\n",
    "    \n",
    "widgets.interactive(visualize_taken_actions, state_index = (0,len(all_states)-1,1), \n",
    "                    agent=widgets.fixed(agent),\n",
    "                    all_states=widgets.fixed(all_states),\n",
    "                    all_actions=widgets.fixed(all_actions)\n",
    "                    \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96934103",
   "metadata": {},
   "source": [
    "## Different learning algorithms\n",
    "\n",
    "We now want to investigate different learning algorithms, therefore we put everything we wrote above into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1404b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class CliffWorld(object):\n",
    "    def __init__(self, grid_length = 12, grid_height = 4):\n",
    "        self.init_state = 0\n",
    "        self.max_steps = 2000\n",
    "        \n",
    "        self.grid_length = grid_length\n",
    "        self.grid_height = grid_height\n",
    "        \n",
    "        self.n_states = self.grid_length * self.grid_height \n",
    "        self.n_actions = 4\n",
    "        \n",
    "        \n",
    "        # initialize state and state-action values\n",
    "        self.state_values = np.zeros(self.n_states)\n",
    "        self.state_action_values = np.zeros((self.n_states,self.n_actions))\n",
    "    \n",
    "    \n",
    "    def run_episode(self, policy):\n",
    "        \n",
    "        \n",
    "        state = self.init_state\n",
    "        all_actions = []\n",
    "        all_states = [state]\n",
    "\n",
    "        \n",
    "        reward_sum = 0\n",
    "\n",
    "        for t in range(self.max_steps):\n",
    "            # choose next action\n",
    "            action = policy()\n",
    "            all_actions.append(action)\n",
    "\n",
    "            # observe outcome of action on environment\n",
    "            next_state = take_action(state, action)\n",
    "            reward = get_reward(next_state)\n",
    "\n",
    "            # sum rewards obtained\n",
    "            reward_sum += reward\n",
    "\n",
    "            if reward == -100 or reward == 0:\n",
    "                break  # episode ends\n",
    "            state = next_state\n",
    "\n",
    "            all_states.append(state)\n",
    "        \n",
    "        self.all_states, self.all_actions, self.reward_sum = all_states, all_actions, reward_sum\n",
    "                    \n",
    "        return all_states, all_actions, reward_sum\n",
    "                    \n",
    "    \n",
    "    def show_last_run_interactive(self):\n",
    "           \n",
    "        \n",
    "        fig, ax, agent, arrow = self.setup_visualization()    \n",
    "        ax.set_title(f'Reward Sum: {self.reward_sum}')\n",
    "        \n",
    "        \n",
    "        wgt = widgets.interactive(self.visualize_taken_actions, state_index = (0,len(self.all_states)-1,1), \n",
    "                agent=widgets.fixed(agent),\n",
    "                arrow=widgets.fixed(arrow),\n",
    "\n",
    "               )\n",
    "        \n",
    "        return wgt\n",
    "\n",
    "    \n",
    "    def show_state_values_in_grid(self, ax = None, min_val = 0, max_val = 0):\n",
    "        \n",
    "        fig, (ax, cax) = plt.subplots(2,1, gridspec_kw={'height_ratios': [15,1]})\n",
    "        \n",
    "        \n",
    "        cmap = mpl.cm.hot\n",
    "        norm = mpl.colors.Normalize(vmin=min_val, vmax=max_val)\n",
    "\n",
    "        self.state_values = -100*np.random.rand(self.n_states)\n",
    "        \n",
    "        ax = self.show_world(ax=ax)\n",
    "        cax.grid(False)\n",
    "        cb = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "        cax.set_title(\"State value\")\n",
    "        for state in range(self.n_states):\n",
    "            state_2d = np.array([state%self.grid_length,state//self.grid_length]).astype(float)\n",
    "            state_2d -= np.array([0.47,0.47])\n",
    "            \n",
    "            # convert the state-value to color \n",
    "            color = cmap(norm(self.state_values[state]))\n",
    "            rect=mpatches.Rectangle(state_2d,width=0.94,height=0.94, \n",
    "                                fill=True,\n",
    "                                color=color,zorder=10)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "    def show_state_action_values_in_grid(self, ax = None, cax = None, min_val = 0, max_val = 0):\n",
    "        \n",
    "        if ax == None or cax == None:\n",
    "            fig, (ax, cax) = plt.subplots(2,1, gridspec_kw={'height_ratios': [15,1]})\n",
    "        \n",
    "        \n",
    "        cmap = mpl.cm.hot\n",
    "        norm = mpl.colors.Normalize(vmin=min_val, vmax=max_val)\n",
    "\n",
    "#         self.state_action_values = -100*np.random.rand(self.n_states,4)\n",
    "        \n",
    "        ax = self.show_world(ax=ax)\n",
    "        cax.grid(False)\n",
    "        cb = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "        cax.set_title(\"State-action value\")\n",
    "        \n",
    "        # the four triangle paths for the four actions\n",
    "        triangle_paths = np.array([\n",
    "            [[0.55,0.5],[0.95,0.95],[0.95,0.05]],\n",
    "            [[0.5,0.45],[0.9,0.05],[0.1,0.05]],\n",
    "            [[0.45,0.5],[0.05,0.05],[0.05,0.9]],\n",
    "            [[0.5,0.55],[0.05,0.95],[0.9,0.95]]           \n",
    "        ])\n",
    "        \n",
    "        for state in range(self.n_states):\n",
    "            \n",
    "            for action in range(4):\n",
    "                \n",
    "                state_2d = np.array([state%self.grid_length,state//self.grid_length]).astype(float)\n",
    "                \n",
    "                path = triangle_paths[action]+state_2d - np.array([0.5,0.5])\n",
    "                # convert the state-value to color \n",
    "                color = cmap(norm(self.state_action_values[state,action]))\n",
    "                \n",
    "                if np.argmax(self.state_action_values[state]) == action:\n",
    "                    ax.scatter(*np.mean(path,axis=0),color=color, edgecolor='k')\n",
    "                else:\n",
    "                    triang=mpatches.Polygon(path,\n",
    "                                    fill=True,\n",
    "                                    color=color,zorder=10)\n",
    "                    \n",
    "                    ax.add_patch(triang)\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    def take_action(self, state, action):\n",
    "        \n",
    "        if action == 0:  # move right\n",
    "            next_state = state + 1\n",
    "            if state % self.grid_length == self.grid_length-1:  # right border\n",
    "                next_state = state\n",
    "\n",
    "        elif action == 1:  # move down\n",
    "            next_state = state - self.grid_length\n",
    "            if state <= self.grid_length-1:  # bottom border\n",
    "                next_state = state\n",
    "\n",
    "        elif action == 2:  # move left\n",
    "            next_state = state - 1\n",
    "            if state % self.grid_length == 0:  # left border\n",
    "                next_state = state\n",
    "\n",
    "        elif action == 3:  # move up\n",
    "            next_state = state + self.grid_length\n",
    "            if state >= self.grid_length * self.grid_height:  # top border\n",
    "                next_state = state\n",
    "\n",
    "        else:\n",
    "            print(\"Action must be between 0 and 3.\")\n",
    "            return None\n",
    "\n",
    "        return int(next_state)\n",
    "    \n",
    "    \n",
    "    def get_reward(self, state):\n",
    "    \n",
    "        if state >= 1 and state < self.grid_length-1: # cliff\n",
    "            return -100\n",
    "        elif state == 11: # goal\n",
    "            return 0\n",
    "        else:\n",
    "            return -1 \n",
    "        \n",
    "    def show_world(self, ax=None):\n",
    "        if ax == None:\n",
    "            fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "\n",
    "\n",
    "        ax.grid(linewidth = 1.4, c='k')\n",
    "        ax.set(\n",
    "            xticks = [0.5+ii for ii in range(-1,12)],\n",
    "            yticks = [0.5 + ii for ii in range(-1,4)],\n",
    "            xlim=[-0.52,11.53],\n",
    "            ylim = [-0.51,3.52])\n",
    "\n",
    "\n",
    "        ax.tick_params(\n",
    "            axis='x',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            bottom=False,      # ticks along the bottom edge are off\n",
    "            top=False,         # ticks along the top edge are off\n",
    "            labelbottom=False) # labels along the bottom edge are off\n",
    "        ax.tick_params(\n",
    "            axis='y',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            left=False,      # ticks along the bottom edge are off\n",
    "            right=False,         # ticks along the top edge are off\n",
    "            labelleft=False) # labels along the bottom edge are off\n",
    "\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        rect=mpatches.Rectangle((0.51,-0.48),9.98,0.96,\n",
    "                                fill=True,\n",
    "                                color=\"#bdbdbd\",zorder=50)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.annotate('S',(0,0), fontweight='bold',fontsize=23,ha='center',va='center', zorder=12)\n",
    "        ax.annotate('G',(11,0), fontweight='bold',fontsize=23,ha='center',va='center', zorder=12)\n",
    "        ax.annotate('The Cliff',(5.5,-0.1), fontsize=23,ha='center',va='center',zorder=200)\n",
    "        return ax\n",
    "        \n",
    "    def visualize_taken_actions(self, state_index, agent, arrow):\n",
    "   \n",
    "        state = self.all_states[state_index]\n",
    "        action = self.all_actions[state_index]\n",
    "\n",
    "        x = state%12\n",
    "        y =state//12\n",
    "\n",
    "        if action == 0:\n",
    "            target_x, target_y = x + 1, y\n",
    "        if action == 1:\n",
    "            target_x, target_y = x, y - 1\n",
    "        if action == 2:\n",
    "            target_x, target_y = x - 1, y\n",
    "        if action == 3:\n",
    "            target_x, target_y = x, y + 1\n",
    "\n",
    "        arrow.xy = (target_x ,target_y )\n",
    "\n",
    "        arrow.set_position([x,y])\n",
    "        agent.set_offsets([x,y])\n",
    "                    \n",
    "    def setup_visualization(self, state = 0):\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,3))\n",
    "        self.show_world(ax)\n",
    "        state_2d = [self.init_state%12,self.init_state//12] \n",
    "        agent = ax.scatter(state_2d[0],state_2d[1],s=800,c='b',zorder=60)\n",
    "        arrow = ax.annotate(\"\",\n",
    "                      xy=(0, 1), xycoords='data',\n",
    "                      xytext=(1, 1), textcoords='data',\n",
    "                      arrowprops=dict(#arrowstyle=\"-\",\n",
    "                                      connectionstyle=\"arc3,rad=0.\",\n",
    "                                      linewidth=4, fc='k'),\n",
    "                            zorder = 100,\n",
    "                            annotation_clip = False\n",
    "                      )\n",
    "\n",
    "        return fig, ax, agent, arrow\n",
    "    \n",
    "    def initiate_values(self, initiation_keyword):\n",
    "        \n",
    "        if initiation_keyword == 'ones':\n",
    "            \n",
    "            self.state_values = np.ones(self.n_states)\n",
    "            self.state_action_values = np.ones((self.n_states,self.n_actions))\n",
    "    \n",
    "    def learn_environment(self,  learning_rule, policy, params, n_episodes,\n",
    "                          initiation_keyword='ones',):\n",
    "               \n",
    "        self.initiate_values(initiation_keyword)\n",
    "\n",
    "        # Run learning\n",
    "        reward_sums = np.zeros(n_episodes)\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Loop over episodes\n",
    "        for episode in range(n_episodes):\n",
    "            state = self.init_state  # initialize state\n",
    "            reward_sum = 0\n",
    "            \n",
    "            next_action = policy(v = self.state_values[state], \n",
    "                                 q = self.state_action_values[state], \n",
    "                                 params = params)\n",
    "\n",
    "            for t in range(self.max_steps):\n",
    "                # choose next action\n",
    "                action = next_action\n",
    "                # observe outcome of action on environment\n",
    "                next_state = take_action(state, action)\n",
    "                reward = get_reward(next_state)\n",
    "                \n",
    "                next_action = policy(v = self.state_values[next_state], \n",
    "                                     q = self.state_action_values[next_state], \n",
    "                                     params = params)\n",
    "                \n",
    "                # update value function\n",
    "                self.state_values, self.state_action_values = learning_rule(\n",
    "                    state, \n",
    "                    action, \n",
    "                    reward, \n",
    "                    next_state, \n",
    "                    next_action, \n",
    "                    self.state_values,\n",
    "                    self.state_action_values, \n",
    "                    params\n",
    "                )\n",
    "\n",
    "                # sum rewards obtained\n",
    "                reward_sum += reward\n",
    "\n",
    "                if reward in [-100, 0]:\n",
    "                    break  # episode ends\n",
    "                state = next_state\n",
    "\n",
    "            reward_sums[episode] = reward_sum\n",
    "\n",
    "        return reward_sums\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ba0ed",
   "metadata": {},
   "source": [
    "We now define again the $\\epsilon$-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6158dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(**kwargs):\n",
    "    \n",
    "    q = kwargs['q']\n",
    "    \n",
    "    if np.random.random() > kwargs['params']['epsilon']:\n",
    "        action = np.argmax(q)\n",
    "    else:\n",
    "        action = np.random.choice(len(q))\n",
    "\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ac82d",
   "metadata": {},
   "source": [
    "## Off-policy vs On-policy\n",
    "\n",
    "So far, we looked at q-learning. It is an off-policy alogorithm, since the update does not depend on the actions the agent actually takes, but on the maximum possible action of the next state. \n",
    "\n",
    "We now compare this approach to the SARSA algorithm, which allows on-policy learning. The learning method in the class is already modified, so that it looks at the decision of the next state before updating the state-action values\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/comp-neural-circuits/intro-to-comp-neuro/raw/dev/notebooks/Exc_12/static/sarsa_vs_qlearning.png\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "Below you can see the implementations of sarsa and q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_learning(state, action, reward, next_state, next_action, state_values, state_action_values, params, *args):\n",
    "    # Q-value of current state-action pair\n",
    "    q = state_action_values[state, action]\n",
    "    \n",
    "    if reward in [-100,0]: # this means the episode ends\n",
    "        next_q = 0\n",
    "    else:\n",
    "        next_q = state_action_values[next_state, next_action]\n",
    "\n",
    "    # write the expression to compute the TD error\n",
    "    td_error = reward + params['gamma'] * next_q - q\n",
    "    # write the expression that updates the Q-value for the state-action pair\n",
    "    state_action_values[state, action] = q + params['alpha'] * td_error\n",
    "    \n",
    "    return state_values, state_action_values # we also return the state_values (although not changed) \n",
    "\n",
    "def q_learning(state, action, reward, next_state, next_action, state_values, state_action_values, params, *args):\n",
    "    # Q-value of current state-action pair\n",
    "    q = state_action_values[state, action]\n",
    "    \n",
    "    if reward in [-100,0]: # this means the episode ends\n",
    "        max_next_q = 0\n",
    "    else:\n",
    "        max_next_q = np.max(state_action_values[next_state])\n",
    "\n",
    "    # write the expression to compute the TD error\n",
    "    td_error = reward + params['gamma'] * max_next_q - q\n",
    "    # write the expression that updates the Q-value for the state-action pair\n",
    "    state_action_values[state, action] = q + params['alpha'] * td_error\n",
    "\n",
    "    return state_values, state_action_values # we also return the state_values (although not changed) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_q_learning, ax_sarsa, cax) = plt.subplots(3,1, gridspec_kw={'height_ratios': [8,8,1]})\n",
    "ax_q_learning.set_title('Q-learning')\n",
    "ax_sarsa.set_title('SARSA')\n",
    "params = {\n",
    "  'epsilon': 0.1,  # epsilon-greedy policy\n",
    "  'alpha': 0.1,  # learning rate\n",
    "  'gamma': 0.8,  # discount factor\n",
    "}\n",
    "\n",
    "np.random.seed(42)\n",
    "cliff_q_learning = CliffWorld()\n",
    "cliff_q_learning.learn_environment(\n",
    "    learning_rule = q_learning, \n",
    "    policy=policy_epsilon_greedy,\n",
    "    params = params, \n",
    "    n_episodes = 2_000,\n",
    "    initiation_keyword='ones',)\n",
    "cliff_q_learning.show_state_action_values_in_grid(ax = ax_q_learning, cax=cax, min_val=-8, max_val = 0)\n",
    "\n",
    "cliff_sarsa = CliffWorld()\n",
    "cliff_sarsa.learn_environment(\n",
    "    learning_rule = sarsa_learning, \n",
    "    policy=policy_epsilon_greedy,\n",
    "    params = params, \n",
    "    n_episodes = 2_000,\n",
    "    initiation_keyword='ones',)\n",
    "cliff_sarsa.show_state_action_values_in_grid(ax = ax_sarsa, cax=cax, min_val=-8, max_val = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96e1c3",
   "metadata": {},
   "source": [
    "## TD($\\lambda$)\n",
    "\n",
    "To we now implement TD($\\lambda$) in the backward view with eligibility traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWorldEligibility(CliffWorld):\n",
    "    def __init__(self, grid_length = 12, grid_height = 4):\n",
    "        super().__init__(grid_length, grid_height)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def learn_environment(self,  learning_rule, policy, params, n_episodes,\n",
    "                          initiation_keyword='ones',lambda_td=0.9):\n",
    "               \n",
    "        self.initiate_values(initiation_keyword)\n",
    "         \n",
    "        # we need to add eligibiity traces for every state\n",
    "        self.eligibility = np.zeros((self.n_states, self.n_actions))\n",
    "        \n",
    "\n",
    "        # Run learning\n",
    "        reward_sums = np.zeros(n_episodes)\n",
    "\n",
    "        # Loop over episodes\n",
    "        for episode in range(n_episodes):\n",
    "            state = self.init_state  # initialize state\n",
    "            \n",
    "            \n",
    "            reward_sum = 0\n",
    "            \n",
    "            next_action = policy(state=state,\n",
    "                                 v = self.state_values[state], \n",
    "                                 q = self.state_action_values[state], \n",
    "                                 params = params)\n",
    "            \n",
    "            print (next_action)\n",
    "            \n",
    "            self.eligibility[state, next_action] = 1\n",
    "            \n",
    "\n",
    "            for t in range(self.max_steps):\n",
    "                # choose next action\n",
    "                action = next_action\n",
    "                # observe outcome of action on environment\n",
    "                next_state = take_action(state, action)\n",
    "                reward = get_reward(next_state)\n",
    "                \n",
    "                next_action = policy(state=next_state,\n",
    "                                    v = self.state_values[next_state], \n",
    "                                     q = self.state_action_values[next_state], \n",
    "                                     params = params)\n",
    "                \n",
    "                # update value function\n",
    "                self.state_values, self.state_action_values = learning_rule(\n",
    "                    state, \n",
    "                    action, \n",
    "                    reward, \n",
    "                    next_state, \n",
    "                    next_action, \n",
    "                    self.state_values,\n",
    "                    self.state_action_values, \n",
    "                    params,\n",
    "                    self.eligibility,\n",
    "                )\n",
    "\n",
    "                # sum rewards obtained\n",
    "                reward_sum += reward\n",
    "\n",
    "                if reward in [-100, 0]:\n",
    "                    break  # episode ends\n",
    "                    \n",
    "                self.eligibility *= lambda_td\n",
    "                self.eligibility[next_state,next_action] = 1\n",
    "                state = next_state\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "            reward_sums[episode] = reward_sum\n",
    "        \n",
    "\n",
    "        return reward_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda_learning(state, action, reward, next_state, next_action, state_values, state_action_values, params, eligibility, *args):\n",
    "    # Q-value of current state-action pair\n",
    "    q = state_action_values[state, action]\n",
    "    \n",
    "    if reward in [-100,0]: # this means the episode ends\n",
    "        next_q = 0\n",
    "    else:\n",
    "        next_q = state_action_values[next_state, next_action]\n",
    "\n",
    "    # write the expression to compute the TD error\n",
    "    td_error = reward + params['gamma'] * next_q - q\n",
    "    # write the expression that updates the Q-value for the state-action pair\n",
    "    state_action_values = state_action_values + params['alpha'] * (td_error * eligibility)\n",
    "    \n",
    "    return state_values, state_action_values # we also return the state_values (although not changed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0801a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_illustration_path(state, **kwargs):\n",
    "    \n",
    "    state_action_dict = {\n",
    "        0 : 3,\n",
    "        12 : 3,\n",
    "        24 : 0,\n",
    "        25 : 0,\n",
    "        26 : 3,\n",
    "        38 : 0,\n",
    "        39 : 0,\n",
    "        40 : 0,\n",
    "        41 : 1,\n",
    "        29 : 2,\n",
    "        28 : 1,\n",
    "        16 : 0,\n",
    "        17 : 0,\n",
    "        18 : 0,\n",
    "        19 : 0,\n",
    "        20 : 3,\n",
    "        32 : 0,\n",
    "        33 : 0,\n",
    "        34 : 0,\n",
    "        35 : 1,\n",
    "        23 : 2,\n",
    "        22 : 1,\n",
    "        10 : 0,\n",
    "        11 : 0,\n",
    "    }\n",
    "    \n",
    "    return state_action_dict[state]\n",
    "\n",
    "params = {\n",
    "  'epsilon': 0.1,  # epsilon-greedy policy\n",
    "  'alpha': 0.1,  # learning rate\n",
    "  'gamma': 0.8,  # discount factor\n",
    "}\n",
    "cliff = CliffWorldEligibility()\n",
    "\n",
    "def run_illustration(lambda_td=0.8):\n",
    "    np.random.seed(18)\n",
    "    cliff.learn_environment(\n",
    "        learning_rule = sarsa_lambda_learning, \n",
    "        policy=policy_illustration_path,\n",
    "        params = params, \n",
    "        n_episodes = 1,\n",
    "        initiation_keyword='ones',\n",
    "        lambda_td = lambda_td)\n",
    "    cliff.show_state_action_values_in_grid(min_val=-8, max_val = 0)\n",
    "\n",
    "widgets.interactive(run_illustration, lambda_td = (0,1,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b09be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "intro-to-comp-neuro",
   "language": "python",
   "name": "intro-to-comp-neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
